\subsection{The NTNU Cyborg Project}
The NTNU cyborg project is a collaboration between several departments at NTNU
including the department of biotechnology, computer and information science,
engineering cybernetics, neuroscience and more. \cite{ntnu_cyborg}
The stated goal for the cyborg project is ``to enable communication between living
nerve tissue and a robot. The social and interactive cyborg will walk around the campus
raising awareness for biotechnology and ICT, bringing NTNU in the forefront of research
and creating a platform for interdisciplinary collaborations and teaching.''
Currently the department of neuroscience is growing neuron cultures which are to
be used as the biological part of the robot.
These neuron cultures are not part of a brain, they are fully dissociated, grown
in special chambers in-vitro.
The robot part of the cyborg has been developed and implemented by the
department of engineering cybernetics, and is currently operational, however it
has not yet been integrated with the in-vitro neuron cultures.
The challenge faced by the cyborg project is creating the infrastructure for
interfacing the neuron cultures and the robot, essentially creating a brain
computer interface.
\subsection{Complex Systems}
\textit{
  In this paper references are made to computations done by both artificial and real
  neurons.
  To make the distinction between these cases clear all computation done by
  computer simulated approximations of neurons will be prefixed as artificial.
}\\
Simply creating a bridge between neural cultures and a machine will not be of any
use without a way to make sense of the information coming from the
network, or how to encode information sent back to the network.
In order to harness the computational power of neural networks a theoretical
framework is therefore necessary, starting with the concept of \textit{complex
 systems}.
Complex systems are dynamic nonlinear systems whose behavior is an emergent
property that cannot be tracked back to a single component.
In these systems positive feedback loops amplify small changes into cascading
changes changing the entire system, while negative feedback loops may cause
other states to be relatively stable, resulting in multiple meta-stable states,
so called \textit{attractors}, that give the system some measure of order.
By classifying neural cultures as one of many cellular computing systems that
exhibit the properties of a complex system we can model the behavior of neurons
with much simpler models.
In \cite{sipper_emergence_1999} Sipper explores cellular computations which
in contrast with microprocessors consist of relatively simple cells which
interact directly only with its neighbors, lacking a global control mechanism.
In short, processors are designed, cellular computation \textit{emerges}.\\
In \cite{langton_computation_1990} Langton explores the requirements for
cellular systems to support \textit{emergent computation}, where
he argues that in order for a system to support emergent computation it must
lie in a \textit{critical phase} between order and chaos, drawing parallels to
phase transitions in material science.
This observation comes from thermodynamics, in which a material may exhibit a
\textit{second order phase transition} between a solid and liquid form where the
material undergoes a continuous transition in contrast to a first order
transition such as melting ice.
Using \textit{cellular automata} as an example, he explores the effect of varying the
rules for calculating the next state with a parameter, $\lambda$, which
describes how likely it is for a cell to enter a \textit{quiescent state}, a state
where it will not disturb other cells until it leaves the quiescent state itself.
At $\lambda = 0$ all cells enter a quiescent state after one step, representing
a fully ordered system, thus at $\lambda = 0$ the system is in the ordered phase.
At $\lambda = 1$ there are no rules that leads to a quiescent state, leading to
very chaotic systems with very high entropy, representing the chaotic phase.
As $\lambda$ is increased from 0 the cellular systems starts to form intricate
structures, increasing in complexity and taking longer to reach a steady state.
This behavior peaks at $\lambda \approx 0.5$ which is the most critical phase in this
particular system, creating complex self-organizing structures.
As $\lambda$ is increased further the self-organizing structures start to give
way to more chaotic and seemingly random behavior, gradually transitioning into
the chaotic phase.\\
It turns out then, that with most cellular systems providing the necessary
conditions for computation to occur is a tractable problem, however harnessing
and shaping this computation is a whole different matter.
In their critical phase systems are unstable but not chaotic which causes them to
be highly sensitive to small changes in topology and initial conditions.
Attempting to maneuver this fitness-landscape by directly changing the
properties of a system would in many ways be like calculating the correct way
for a butterfly in china to flap its wings in order to cause a hurricane in New York.\\
\subsection{Reservoir Computing}
Faced with the intractability of designing cellular computation systems in a top
down manner, and the unpredictable relation between cause and effect in self organizing
systems it is necessary to approach complex systems in a different manner.
Seemingly, classifying the neural culture as a complex system has not provided
any useful tools for understanding how to interact with it, on the contrary it
makes such a task look futile.
However, in computer science the recent field of \textit{reservoir computing}
has emerged, embracing the complexity and unpredictability of certain complex
systems.
In reservoir computing, a complex systems is used as a \textit{reservoir}
\cite{schrauwen_overview_2007} which
``acts as a complex nonlinear dynamic filter that transforms the
input signals using a high-dimensional temporal map, not unlike the operation
of an explicit, temporal kernel function.''\\
In simpler terms, the properties that make complex systems so hard to work with
such as sensitivity to initial conditions also allows them to discern very
subtle nuances in input, and their complex behavioral patterns causes the
systems to change their behavior to new input based on previous input.
Many such reservoirs have been successfully exploited:
In \cite{jaeger_adaptive_2003} an \textit{echo state network} 
is utilized to solve classification problems.
More esoteric reservoirs have been used, for instance in
\cite{natschlager_liquid_2002} the idea of reservoir computing is taken quite
literally using a bucket of water as a reservoir.
With reservoir computing there is no need for a designer shaping a system, the
systems organize and shape their own behavior, all that needs (and can) be done
is to find a good way to present information to the reservoir as either initial
conditions or as perturbations, and how to interpret the resulting dynamics.
From this perspective using a neural culture is simplified into
learning some approximation of the correlation between input and output from the
reservoir, treating the internals as a black box.
Like the servants of the oracle of Delphi, the reservoir computing system must
phrase the question correctly, and interpret the often incomprehensible answer.
\ref{fig:RC} shows a typical RC (reservoir computing) system with three inputs
and two outputs. The inputs are processed in a simple feed-forward neural
network before perturbing the reservoir in some way.
Similarly the state of the reservoir is being processed by an output layer
before leaving the RC system.
In the figure the input and output processing is done by feed forward neural
networks, but we note that this is only one of many possible filters.
Inputs 1, 2 and 3 are snapshots of the current state of the problem we attempt
to solve with reservoir computing.
Since our filters have no state, at least not beyond some time horizon we see
that the history of the system must in some way be encoded in the reservoir in
order for the RC system to solve problems in scope wider than the limited amount
of state that may be contained in the filters.
\begin{figure*}[h]
  \input{latex/tikz}
  \caption{An idealized reservoir computing system.
  The square boxes are simple filters, in this particular case represented by
  feed forward neural networks.
  The input processing and output layer are both trained with machine learning
  to process input and to interpret output from a reservoir in order to perform
  a specific task.}
  \label{fig:RC}
\end{figure*}
\subsection{Neuron Computing}
Neurons are cells which communicate with each others using electro-chemical
signals in vast interconnected networks.
These networks communicate in a complex interplay between neurotransmitters and voltage
spikes, and their communication prompt structural changes in the network,
deeply intertwining the emergent behavior and self organizing properties on neurons.
There is a high degree of correlation between the electrical, chemical signals
in these neural network, thus a necessary simplification of considering only the
electrical signals when interacting with neural networks can be made at a small
cost of information loss.
These electrical signals, known as action potentials, are caused
by polarity differentials between the ions inside and the ions outside of the
cell membrane.
A neuron maintains this potential difference by transporting ions of opposite
polarity through the cell membrane, essentially preparing a spike of electrical
activity.
This spike is triggered as the neuron receives electrical stimuli via a network
of electrical receptors called dendrites.
When excited, a neuron which has built up a sufficient polarity difference will
trigger a polarity shift.
This polarity shift cascades along the axon, a long tendril that can extend to
reach far away neurons, forming branches that may reach multiple neurons.
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End: