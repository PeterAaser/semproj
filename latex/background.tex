\subsection{The NTNU Cyborg Project}
The NTNU cyborg project is a collaboration between several departments at NTNU
including the department of biotechnology, computer and information science,
engineering cybernetics, neuroscience and more. \cite{ntnu_cyborg_web}
The stated goal for the cyborg project is ``to enable communication between living
nerve tissue and a robot. The social and interactive cyborg will walk around the campus
raising awareness for biotechnology and ICT, bringing NTNU in the forefront of research
and creating a platform for interdisciplinary collaborations and teaching.''
Currently the department of neuroscience is growing neuron cultures which are to
be used as the biological part of the robot.
These neuron cultures are not part of a brain, they are fully dissociated, grown
in special chambers in-vitro.
The robot part of the cyborg has been developed and implemented by the
department of engineering cybernetics, and is currently operational, however it
has not yet been integrated with the in-vitro neuron cultures.
The challenge faced by the cyborg project is the infrastructure for interfacing the
neuron cultures and the robot, essentially creating a brain computer interface.
\subsection{Complex Systems}
\textit{
  In this paper references are made to computations done by both artificial and real
  neurons.
  To make the distinction between these cases clear all computation done by
  computer simulated approximations of neurons will be prefixed as artificial.
}\\
Simply creating a bridge between neural cultures and a machine will not of any
use without a way to understand what to make of the information coming from the
network, and how to encode information sent back to the network.
In order to harness the computational power of neural networks a theoretical
framework is therefore necessary, starting with the concept of \textit{complex
  systems}.
The self-organizing properties that makes cellular computing such as
neural networks so successful in nature also make understanding how they work
very difficult, there are no simple relations between cause and effect.
In contrast with microprocessors, cellular computing describes systems where
each component cell is relatively simple, interacting mostly with its neighbors
with no global control mechanism.
Neuron cultures pose additional difficulties compared to simpler cellular
systems, both practical and complexity-wise, as even a single neuron is a vastly
complex entity.
To approach neuron cultures we therefore identify a set of fundamental
properties common for cellular computing systems so that we may employ simpler
models first.
The properties common for the systems we explore are that they are complex
systems: They are self organizing and have nonlinear responses, displaying
behavior where the sum is greater than the parts.
Many such systems have been explored, such as recurrent artificial neural
networks \cite{bertschinger_real-time_2004}, random boolean networks
\cite{gershenson_introduction_2004} and cellular automata.
In \cite{langton_computation_1990} Langton explores the requirements for systems
to support emergent computation, where
he argues that in order for a system to support emergent computation it must
lie in a \textit{critical phase} between order and chaos, drawing parallels to
phase transitions in material science.
This observation comes from thermodynamics, in which a material may exhibit a
\textit{second order phase transition} between a solid and liquid form where the
material undergoes a continuous transition in contrast to a first order
transition such as melting ice.
Using cellular automata as an example, he explores the effect of varying the
rules for calculating the next state with a parameter, $\lambda$, which
describes how likely it is for a cell to enter a \textit{quiescent state}, a state
where it will not disturb other cells until it leaves the quiescent state itself.
At $\lambda = 0$ all cells enter a quiescent state after one step, representing
a fully ordered system, thus at $\lambda = 0$ the system is in the ordered phase.
At $\lambda = 1$ there are no rules that leads to a quiescent state, leading to
very chaotic systems with very high entropy, representing the chaotic phase.
As $\lambda$ is increased from 0 the cellular systems starts to form intricate
structures, increasing in complexity and taking longer to reach a steady state.
This behavior peaks at $\lambda \approx 0.5$ which is the most critical phase in this
particular system, creating complex self-organizing structures.
As $\lambda$ is increased further the self-organizing structures start to give
way to more chaotic and seemingly random behavior, gradually transitioning into
the chaotic phase.\\
It turns out then, that with most cellular systems providing the necessary
conditions for computation to occur is a tractable problem, however harnessing
and shaping this computation is a whole different matter.
In their critical phase systems are unstable but not chaotic which causes them to
be highly sensitive to small changes in topology and initial conditions.
Attempting to maneuver this fitness-landscape by directly changing the
properties of a system would in many ways be like calculating the correct way
for a butterfly in china to flap its wings in order to cause a hurricane in New York.\\
\subsection{Reservoir Computing}
Faced with the intractability of designing cellular computation systems in a top
down manner, and the unpredictable relation between cause and effect in self organizing
systems has made it necessary to approach complex systems in a different manner.
One such approach is to treat the system as a \textit{reservoir}
\cite{schrauwen_overview_2007} which
``acts as a complex nonlinear dynamic filter that transforms the
input signals using a high-dimensional temporal map, not unlike the operation
of an explicit, temporal kernel function.''\\
In reservoir computing there is no designer shaping a system, the systems
organize and shape their own behavior, forming systems with complex behavior
that can reflect subtle differences in initial conditions and stimuli.
From the perspective of reservoir computing using a neural culture is reduced to
learning the correlation between input and output from the reservoir, treating
the internals as a black box.
\ref{fig:RC} shows a typical RC (reservoir computing) system with three inputs
and two outputs. The inputs are processed in a simple feed-forward neural
network before perturbing the reservoir in some way.
Similarly the state of the reservoir is being processed by an output layer
before leaving the RC system.
In the figure the input and output processing is done by feed forward neural
networks, but we note that this is only one of many possible filters.
Inputs 1, 2 and 3 are snapshots of the current state of the problem we attempt
to solve with reservoir computing.
Since our filters have no state, at least not beyond some time horizon we see
that the history of the system must in some way be encoded in the reservoir in
order for the RC system to solve problems in scope wider than the limited amount
of state that may be contained in the filters.
\begin{figure*}[h]
  \input{latex/tikz}
  \caption{An idealized reservoir computing system.
  The square boxes are simple filters, in this particular case represented by
  feed forward neural networks.
  The input processing and output layer are both trained with machine learning
  to process input and to interpret output from a reservoir in order to perform
  a specific task.
}
  \label{fig:RC}
\end{figure*}
\subsection{Neuron Computing}
Neurons are cells which communicate with each others using electro-chemical
signals in vast interconnected networks.
These networks communicate in a complex interplay between neurotransmitters and voltage
spikes, and their communication prompt structural changes in the network,
deeply intertwining the emergent behavior and self organizing properties on neurons.
There is a high degree of correlation between the electrical, chemical signals
in these neural network, thus a necessary simplification of considering only the
electrical signals when interacting with neural networks can be made at a small
cost of information loss.
These electrical signals, known as action potentials, are caused
by polarity differentials between the ions inside and the ions outside of the
cell membrane.
A neuron maintains this potential difference by transporting ions of opposite
polarity through the cell membrane, essentially preparing a spike of electrical
activity.
This spike is triggered as the neuron receives electrical stimuli via a network
of electrical receptors called dendrites
When excited, a neuron which has built up a sufficient polarity difference will
trigger a polarity shift.
This polarity shift cascades along the axon, a long tendril that can extend to
reach far away neurons, forming branches that may reach multiple neurons.\\
In artificial neural networks consisting of primitive nodes emulating the
electrical signaling of neurons has been utilized for hard tasks such as image
recognition and classification tasks, not really sure where I'm going with this.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End: